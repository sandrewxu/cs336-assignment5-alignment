{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767276a2",
   "metadata": {},
   "source": [
    "Example Code from Section 4: SFT\n",
    "\n",
    "## 4.1 Using HuggingFace Models\n",
    "Loading a HuggingFace model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab3f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-Math-1.5B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Math-1.5B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f04146",
   "metadata": {},
   "source": [
    "**Forward pass.** We run a forward pass on a batch of input IDs and get the logits with the .logits attribute of the output. Then, we can compute the loss between the model's predicted logits and the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c81fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = train_batch[\"input_ids\"].to(device)\n",
    "labels = train_batch[\"labels\"].to(device)\n",
    "\n",
    "logits = model(input_ids).logits    # does this compute only next token or a whole sequence? slightly confused\n",
    "loss = F.cross_entropy(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1feb64e",
   "metadata": {},
   "source": [
    "**Saving a trained model.** Use the .save_pretrained() function to save a model to a directory after training is finished. Also recommend saving the tokenizer as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_directory=output_dir)\n",
    "tokenizer.save_pretrained(save_directory=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92264b7e",
   "metadata": {},
   "source": [
    "**Gradient accumulation.** 80GB GPU does not have enough memory to support reasonable batch sizes. To use larger batch sizes, we use gradient accumulation. Rather than updating our model weights after every batch, we accumulate the gradients over several batches before taking a step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 4\n",
    "for idx, (inputs, labels) in enumerate(data_loader):\n",
    "    # Forward pass\n",
    "    logits = model(inputs)\n",
    "    loss = loss_fn(logits, labels) / gradient_accumulation_steps    # why do we divide loss by steps?\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward() # this accumulates gradients?\n",
    "\n",
    "    if (idx + 1) % gradient_accumulation_steps == 0:\n",
    "        # update weights and zero gradients every `gradient_accumulation_steps` batches\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bdcf66",
   "metadata": {},
   "source": [
    "## 4.3 SFT Experiment\n",
    "\n",
    "Here is some starter code to initialize vLLM and to load the policy weights into the vLLM instance before every rollout phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c8773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from vllm import LLM\n",
    "from vllm.model_executor import set_random_seed as vllm_set_random_seed\n",
    "\n",
    "def init_vllm(\n",
    "    model_id: str,\n",
    "    device: str,\n",
    "    seed: int,\n",
    "    gpu_memory_utilization: float = 0.85,\n",
    "):\n",
    "    \"\"\"\n",
    "    Start the inference process, here we use vLLM to hold a model on\n",
    "    a GPU separate from the policy.\n",
    "    \"\"\"\n",
    "    vllm_set_random_seed(seed)\n",
    "\n",
    "    world_size_patch = patch(\"torch.distributed.get_world_size\", return_value=1)\n",
    "    profiling_patch = patch(\n",
    "        \"vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling\",\n",
    "        return_value=None,\n",
    "    )\n",
    "    with world_size_patch, profiling_patch:\n",
    "        return LLM(\n",
    "            model=model_id,\n",
    "            device=device,\n",
    "            dtype=torch.bfloat16,\n",
    "            enable_prefix_caching=True,\n",
    "            gpu_memory_utilization=gpu_memory_utilization,\n",
    "        )\n",
    "\n",
    "def load_policy_into_vllm_instance(policy: PreTrainedModel, llm: LLM):\n",
    "    \"\"\"\n",
    "    Copied from https://github.com/huggingface/trl/blob/\n",
    "    22759c820867c8659d00082ba8cf004e963873c1/trl/trainer/grpo_trainer.py#L670\n",
    "    \"\"\"\n",
    "    state_dict = policy.state_dict()\n",
    "    llm_model = llm.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "    llm_model.load_weights(state_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6270f4",
   "metadata": {},
   "source": [
    "You may find it helpful to log metrics with respect to both the train and validation steps (this will also be useful in later RL experiments). To do this in wandb, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff60599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Setup wandb metrics\n",
    "wandb.define_metric(\"train_step\")   # x-axis for training\n",
    "wandb.define_metric(\"eval_step\")    # x-axis for eval\n",
    "\n",
    "# everything that starts with train / is tied to train_step\n",
    "wandb.define_metric(\"train/*\", step_metric=\"train_step\")\n",
    "\n",
    "# everything that starts with eval / is tied to eval_step\n",
    "wandb.define_metric(\"eval/*\", step_metric=\"eval_step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d56108",
   "metadata": {},
   "source": [
    "Use gradient clipping with clip value 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ac0a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
